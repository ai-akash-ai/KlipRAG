{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36969b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857918e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 10:55:38.400 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:38.864 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:38.866 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.055 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\akash\\OneDrive\\Desktop\\Agents\\agents\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-08 10:55:39.056 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.057 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.058 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-08 10:55:39.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.060 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.062 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.065 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.067 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.068 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.069 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.070 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.072 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.073 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.075 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 10:55:39.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported components from app_history.py\n"
     ]
    }
   ],
   "source": [
    "# Import components from app_history.py\n",
    "# We'll import directly from your original file to ensure we're using the same components\n",
    "# Make sure app_history.py is in the same directory or in your Python path\n",
    "try:\n",
    "    # Try to import without modifying the file\n",
    "    sys.path.append('.')  # Add current directory to path\n",
    "    from app_history import (\n",
    "        llm, retrieval_chain, vector_store\n",
    "    )\n",
    "    print(\"✅ Successfully imported components from app_history.py\")\n",
    "except ImportError:\n",
    "    # If direct import fails, we'll recreate the necessary components\n",
    "    print(\"⚠️ Could not import directly from app_history.py\")\n",
    "    print(\"Recreating necessary components...\")\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    \n",
    "    if not MONGO_URI or not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"Please ensure MONGO_URI and GOOGLE_API_KEY are set in your .env file\")\n",
    "    \n",
    "    # Recreate the components from app_history.py\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "    from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "    from pymongo import MongoClient\n",
    "    from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "    from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    \n",
    "    # Initialize embeddings and MongoDB connection\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/gemini-embedding-exp-03-07\",\n",
    "        task_type=\"RETRIEVAL_QUERY\"\n",
    "    )\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    collection = client[\"bem\"][\"flattened_expenses_googleai\"]\n",
    "    \n",
    "    vector_store = MongoDBAtlasVectorSearch(\n",
    "        collection=collection,\n",
    "        embedding=embeddings,\n",
    "        index_name=\"receipts_vector_index\"\n",
    "    )\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Contextual retriever chain\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"human\", \"Given the conversation history, reformulate this as a standalone query about expenses:\"),\n",
    "    ])\n",
    "    \n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, retriever_prompt\n",
    "    )\n",
    "    \n",
    "    # Answer generation chain\n",
    "    system_prompt = \"\"\"You are a smart expense assistant. Use these receipts and conversation history:\n",
    "    \n",
    "    Receipts:\n",
    "    {context}\n",
    "    \n",
    "    Conversation History:\n",
    "    {chat_history}\"\"\"\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    \n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    retrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    print(\"✅ Successfully recreated components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edea5768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exploring the expense data ---\n",
      "Found 2 sample receipts\n",
      "Sample receipt excerpt:\n",
      "Expense at Paragon for 2500 AED on 2025-03-11 under Meals category. Description: Meal with family....\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Exploring the expense data ---\")\n",
    "# Sample a few receipts to understand what data we have\n",
    "sample_docs = vector_store.similarity_search(\"show sample receipts\", k=2)\n",
    "print(f\"Found {len(sample_docs)} sample receipts\")\n",
    "print(\"Sample receipt excerpt:\")\n",
    "if sample_docs:\n",
    "    print(sample_docs[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6880fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining test cases ---\n",
      "Created 5 test cases\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Defining test cases ---\")\n",
    "# These should be customized based on your actual receipt data\n",
    "question_answers = [\n",
    "    {\n",
    "        'question': \"What was my most expensive purchase last month?\",\n",
    "        'answer': 'The most expensive purchase last month was X for $Y.',\n",
    "        'chat_history': []\n",
    "    },\n",
    "    {\n",
    "        'question': \"How much did I spend on dining?\",\n",
    "        'answer': 'You spent $X on dining expenses.',\n",
    "        'chat_history': []\n",
    "    },\n",
    "    {\n",
    "        'question': \"Do I have any receipts from Target?\",\n",
    "        'answer': 'Yes/No, you have X receipts from Target.',\n",
    "        'chat_history': []\n",
    "    },\n",
    "    {\n",
    "        'question': \"What about Amazon?\",\n",
    "        'answer': 'You have X purchases from Amazon.',\n",
    "        'chat_history': [\n",
    "            HumanMessage(content=\"Do I have any receipts from Target?\"),\n",
    "            AIMessage(content=\"Yes, you have several receipts from Target.\")\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'question': \"Which category did I spend the most on?\",\n",
    "        'answer': 'You spent the most on X category, totaling $Y.',\n",
    "        'chat_history': []\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(question_answers)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49431e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating answers ---\n",
      "Processing question 1: What was my most expensive purchase last month?\n",
      "Answer: Last month (April 2025) your most expensive purchase was office supplies from NASCO for 400 AED....\n",
      "Processing question 2: How much did I spend on dining?\n",
      "Answer: You spent a total of 2782 AED on dining....\n",
      "Processing question 3: Do I have any receipts from Target?\n",
      "Answer: No, based on the receipts I have, there are no expenses recorded from Target.  I have receipts from ...\n",
      "Processing question 4: What about Amazon?\n",
      "Answer: I don't see any receipts from Amazon in your expense history.  Would you like me to check for someth...\n",
      "Processing question 5: Which category did I spend the most on?\n",
      "Answer: Your top spending category is Meals, totaling 2658 AED....\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using the retrieval chain\n",
    "print(\"\\n--- Generating answers ---\")\n",
    "predictions = []\n",
    "\n",
    "for i, qa_pair in enumerate(question_answers):\n",
    "    print(f\"Processing question {i+1}: {qa_pair['question']}\")\n",
    "    \n",
    "    # Get prediction from the retrieval chain\n",
    "    response = retrieval_chain.invoke({\n",
    "        \"input\": qa_pair[\"question\"],\n",
    "        \"chat_history\": qa_pair.get(\"chat_history\", [])\n",
    "    })\n",
    "    \n",
    "    # Extract the answer\n",
    "    prediction = {\"question\": qa_pair[\"question\"], \"result\": response[\"answer\"]}\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    print(f\"Answer: {prediction['result'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ae3bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating answers ---\n"
     ]
    }
   ],
   "source": [
    "# Create an evaluation chain using LLM\n",
    "print(\"\\n--- Evaluating answers ---\")\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Grade the predictions\n",
    "graded_outputs = eval_chain.evaluate(\n",
    "    question_answers, \n",
    "    predictions,\n",
    "    question_key=\"question\",\n",
    "    prediction_key=\"result\",\n",
    "    answer_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b51b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "\n",
      "Question 1: What was my most expensive purchase last month?\n",
      "Expected: The most expensive purchase last month was X for $Y.\n",
      "Actual: Last month (April 2025) your most expensive purchase was office supplies from NASCO for 400 AED.\n",
      "Evaluation: {'results': 'GRADE: INCORRECT'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 2: How much did I spend on dining?\n",
      "Expected: You spent $X on dining expenses.\n",
      "Actual: You spent a total of 2782 AED on dining.\n",
      "Evaluation: {'results': 'GRADE: INCORRECT'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 3: Do I have any receipts from Target?\n",
      "Expected: Yes/No, you have X receipts from Target.\n",
      "Actual: No, based on the receipts I have, there are no expenses recorded from Target.  I have receipts from ...\n",
      "Evaluation: {'results': 'GRADE: INCORRECT'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 4: What about Amazon?\n",
      "Expected: You have X purchases from Amazon.\n",
      "Actual: I don't see any receipts from Amazon in your expense history.  Would you like me to check for someth...\n",
      "Evaluation: {'results': 'GRADE: INCORRECT'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 5: Which category did I spend the most on?\n",
      "Expected: You spent the most on X category, totaling $Y.\n",
      "Actual: Your top spending category is Meals, totaling 2658 AED.\n",
      "Evaluation: {'results': 'GRADE: INCORRECT'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the graded outputs safely\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "for i, (qa_pair, prediction, graded_output) in enumerate(zip(question_answers, predictions, graded_outputs)):\n",
    "    print(f\"\\nQuestion {i+1}: {qa_pair['question']}\")\n",
    "    print(f\"Expected: {qa_pair['answer']}\")\n",
    "    print(f\"Actual: {prediction['result'][:100]}...\" if len(prediction['result']) > 100 else f\"Actual: {prediction['result']}\")\n",
    "    \n",
    "    # Try to extract evaluation text from possible keys\n",
    "    eval_text = graded_output.get(\"text\") or graded_output.get(\"result\") or str(graded_output)\n",
    "    print(f\"Evaluation: {eval_text}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a130569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary ---\n",
      "Total questions: 5\n",
      "Correct answers: 5\n",
      "Success rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate success rate\n",
    "correct_answers = sum(\n",
    "    1 for output in graded_outputs\n",
    "    if \"correct\" in (output.get(\"text\") or output.get(\"result\") or str(output)).lower()\n",
    ")\n",
    "total_questions = len(graded_outputs)\n",
    "success_rate = correct_answers / total_questions if total_questions > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Summary ---\")\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "print(f\"Success rate: {success_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3fe9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_prompt = f\"\"\"\n",
    "You are evaluating an AI expense assistant that answered {total_questions} questions with a success rate of {success_rate:.2%}.\n",
    "\n",
    "Here are the questions and evaluations:\n",
    "{[{\n",
    "  \"question\": qa[\"question\"],\n",
    "  \"expected\": qa[\"answer\"],\n",
    "  \"actual\": pred[\"result\"],\n",
    "  \"evaluation\": (grade.get(\"text\") or grade.get(\"result\") or str(grade))\n",
    "} for qa, pred, grade in zip(question_answers, predictions, graded_outputs)]}\n",
    "\n",
    "Based on these results, provide:\n",
    "1. Overall assessment of the assistant's performance\n",
    "2. Specific strengths observed\n",
    "3. Common error patterns or weaknesses\n",
    "4. Recommendations for improving the assistant\n",
    "\n",
    "Be specific and actionable in your recommendations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a7fb72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Overall Assessment\n",
      "\n",
      "Despite a reported 100% success rate, the AI expense assistant performed poorly.  All five evaluated questions were marked \"INCORRECT\" according to the provided evaluations. This discrepancy suggests a serious flaw in the automatic evaluation process.  The assistant demonstrates some understanding of the questions and provides relevant information, but it fails to consistently meet the expected response format or provide entirely accurate information.\n",
      "\n",
      "## Specific Strengths Observed\n",
      "\n",
      "* **Understanding User Intent:** The assistant correctly interprets the intent behind each question. It understands queries about highest expenses, specific vendor expenses, and category spending.\n",
      "* **Information Retrieval:**  The assistant seems to access and process expense data, retrieving relevant information like vendor names, amounts, and categories.\n",
      "* **Natural Language Generation:** The assistant generates human-readable responses that are generally well-structured and grammatically correct.\n",
      "\n",
      "## Common Error Patterns/Weaknesses\n",
      "\n",
      "* **Mismatch with Expected Format:** The primary weakness is the inconsistency between the actual responses and the predefined expected format.  For example, the expected format often includes placeholder variables like $X and $Y, which the assistant doesn't utilize. This suggests a rigid evaluation metric that penalizes correct information presented in a slightly different format.\n",
      "* **Potential Data Accuracy Issues:** While the format is the main issue, there's a possibility of minor inaccuracies in the data itself. For instance, the assistant mentions \"Meals\" totaling 2658 AED, while another response mentions \"dining\" expenses of 2782 AED. This discrepancy needs further investigation.\n",
      "* **Lack of Currency Consistency:** The expected responses use dollars ($), while the actual responses use AED. This inconsistency should be addressed for a better user experience.\n",
      "\n",
      "## Recommendations for Improving the Assistant\n",
      "\n",
      "1. **Revise Evaluation Metrics:** The current evaluation process is clearly flawed.  Instead of relying on rigid string matching with placeholders, implement a more semantic evaluation that focuses on the accuracy of the information conveyed, regardless of minor phrasing differences.  Consider using metrics like F1-score or BLEU score for evaluating the semantic similarity between the expected and actual responses.\n",
      "\n",
      "2. **Standardize Currency:**  Choose a default currency (e.g., the user's preferred currency) and ensure consistency between the expected responses and the assistant's output.  Provide an option for users to switch currencies if needed.\n",
      "\n",
      "3. **Improve Data Accuracy:** Investigate the potential discrepancies in reported amounts (e.g., \"Meals\" vs. \"dining\").  Ensure the assistant accesses the correct and most up-to-date expense data.\n",
      "\n",
      "4. **Enhance Response Formatting:** Train the assistant to provide answers in a more structured and consistent format. While flexibility is important, providing key information in a predictable manner improves usability.  Consider using templates or structured output formats (e.g., JSON) for easier parsing and integration with other systems.\n",
      "\n",
      "5. **Implement Unit Testing:**  Develop a comprehensive suite of unit tests to cover various expense-related scenarios and edge cases.  This will help identify and fix bugs early in the development process and ensure the assistant's accuracy and robustness.\n",
      "\n",
      "6. **User Feedback and Iteration:**  Gather user feedback on the assistant's performance and use this feedback to iteratively improve its accuracy and usability.  A/B testing different response formats and phrasing can help identify the most effective approaches.\n",
      "\n",
      "\n",
      "By addressing these issues, the AI expense assistant can become a truly valuable tool for managing finances.\n"
     ]
    }
   ],
   "source": [
    "feedback = llm.invoke(feedback_prompt)\n",
    "print(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b956e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

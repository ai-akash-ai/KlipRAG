{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84ab606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9955facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c353cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Setting Up the Vector Store and LLM\n",
      "This mirrors the setup in app_history.py\n"
     ]
    }
   ],
   "source": [
    "if not MONGO_URI or not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please ensure MONGO_URI and GOOGLE_API_KEY are set in your .env file\")\n",
    "\n",
    "print(\"## Setting Up the Vector Store and LLM\")\n",
    "print(\"This mirrors the setup in app_history.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e692e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and MongoDB connection\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\",\n",
    "    task_type=\"RETRIEVAL_QUERY\"\n",
    ")\n",
    "client = MongoClient(MONGO_URI)\n",
    "collection = client[\"bem\"][\"flattened_expenses_googleai\"]\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    collection=collection,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"receipts_vector_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f604bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Creating the Retrieval Chain\n",
      "This recreates the same chain used in the main application\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"\\n## Creating the Retrieval Chain\")\n",
    "print(\"This recreates the same chain used in the main application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c64709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual retriever chain\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"human\", \"Given the conversation history, reformulate this as a standalone query about expenses:\"),\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, retriever_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457ecd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Sample Data for Evaluation\n",
      "First, let's look at what kinds of receipts we have in the database\n"
     ]
    }
   ],
   "source": [
    "# Answer generation chain\n",
    "system_prompt = \"\"\"You are a smart expense assistant. Use these receipts and conversation history:\n",
    "\n",
    "Receipts:\n",
    "{context}\n",
    "\n",
    "Conversation History:\n",
    "{chat_history}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "retrieval_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "print(\"\\n## Sample Data for Evaluation\")\n",
    "print(\"First, let's look at what kinds of receipts we have in the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4833d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample receipt data:\n",
      "\n",
      "RECEIPT 1:\n",
      "Expense at Paragon for 2500 AED on 2025-03-11 under Meals category. Description: Meal with family.\n",
      "\n",
      "RECEIPT 2:\n",
      "Expense at KFC for 98 AED on 2025-04-10 under Meals category. Description: Farewell party\n",
      "\n",
      "RECEIPT 3:\n",
      "Expense at magnati for 24 AED on 2025-01-29 under Meals category. Description: Client lunch meeting with XYZ.\n"
     ]
    }
   ],
   "source": [
    "# First, let's look at what kinds of receipts we have in the database\n",
    "sample_docs = vector_store.similarity_search(\"show me sample receipts\", k=3)\n",
    "print(\"Sample receipt data:\")\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    print(f\"\\nRECEIPT {i+1}:\")\n",
    "    print(doc.page_content[:500] + \"...\" if len(doc.page_content) > 500 else doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90eb88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Evaluation Function\n",
      "Creating a function to evaluate the AI's responses\n"
     ]
    }
   ],
   "source": [
    "# Define test cases based on what's in the receipts\n",
    "# Note: You should customize these based on your actual receipt data\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"What was my most expensive purchase last month?\",\n",
    "        \"expected_answer\": \"Based on the receipts, your most expensive purchase last month was...\",\n",
    "        \"chat_history\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How much did I spend on dining in total?\",\n",
    "        \"expected_answer\": \"According to the receipts, you spent a total of...\",\n",
    "        \"chat_history\": []\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Did I make any purchases at Target?\",\n",
    "        \"expected_answer\": \"Yes, you made purchases at Target...\",\n",
    "        \"chat_history\": []\n",
    "    },\n",
    "    # Conversation with history\n",
    "    {\n",
    "        \"question\": \"What about at Amazon?\",\n",
    "        \"expected_answer\": \"Based on the receipts, you...\",\n",
    "        \"chat_history\": [HumanMessage(content=\"Did I make any purchases at Target?\"), \n",
    "                       AIMessage(content=\"Yes, you made purchases at Target...\")]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which category did I spend the most on?\",\n",
    "        \"expected_answer\": \"Based on the receipts, you spent the most on...\",\n",
    "        \"chat_history\": []\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n## Evaluation Function\")\n",
    "print(\"Creating a function to evaluate the AI's responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd81c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Run Evaluation\n",
      "Running the evaluation on our test cases\n",
      "\n",
      "--- Evaluating Test Case 1 ---\n",
      "Question: What was my most expensive purchase last month?\n",
      "\n",
      "AI Response: Last month (April 2025) your most expensive purchase was office supplies from NASCO for 400 AED.\n",
      "\n",
      "Evaluation: \n",
      "Relevance: 5 - The response directly addresses the question of the most expensive purchase last month.\n",
      "\n",
      "Accuracy: 5 - Assuming the provided information about the NASCO purchase is correct, the response accurately identifies the most expensive purchase.\n",
      "\n",
      "Completeness: 5 - The response provides the month, vendor, item purchased, and cost, which constitutes a complete answer to the question.\n",
      "\n",
      "Clarity: 5 - The response is clear, concise, and easy to understand.  It uses simple language and provides all the necessary information in a straightforward manner.\n",
      "\n",
      "Overall: 5 - The AI assistant's response is excellent. It accurately and completely answers the question in a clear and concise way, perfectly matching the expected answer pattern.\n",
      "\n",
      "--- Evaluating Test Case 2 ---\n",
      "Question: How much did I spend on dining in total?\n",
      "\n",
      "AI Response: You spent a total of 2782 AED on dining.\n",
      "\n",
      "Evaluation: \n",
      "Relevance: 5 - The response directly addresses the question of how much was spent.\n",
      "Accuracy: 4 - We don't know if 2782 AED is the correct amount without seeing the receipts.  It follows the expected pattern, but accuracy can't be fully confirmed.\n",
      "Completeness: 4 - It states the total amount and the category (dining), which is good. However, it could be more complete by mentioning the time period (e.g., \"this month,\" \"last week\").\n",
      "Clarity: 5 - The response is clear, concise, and easy to understand.\n",
      "\n",
      "Overall: 4 - The response is good and mostly follows the expected pattern.  The lack of verifiable accuracy and a specified time frame slightly lowers the overall score.\n",
      "\n",
      "--- Evaluating Test Case 3 ---\n",
      "Question: Did I make any purchases at Target?\n",
      "\n",
      "AI Response: No, based on the receipts I have, you didn't make any purchases at Target.  Your recorded expenses were at H&M, Faber Castle, Staedlar, NASCO, and Dominos.\n",
      "\n",
      "Evaluation: \n",
      "Relevance: 5 - The response directly addresses the question about purchases at Target.  It provides a clear \"no\" and then elaborates on where spending did occur.\n",
      "\n",
      "Accuracy: 1 - The response contradicts the expected answer, which indicates there *were* purchases at Target.  The AI is providing incorrect information.\n",
      "\n",
      "Completeness: 5 - While inaccurate, the response is complete in that it provides an answer and lists alternative stores where purchases were made. This adds context to the \"no.\"\n",
      "\n",
      "Clarity: 5 - The response is clear, concise, and easy to understand. The language is straightforward, and the listed stores are easily recognizable.\n",
      "\n",
      "Overall: 3 - While the response is relevant, clear, and complete, its inaccuracy significantly lowers its overall value.  A correct answer is paramount for an expense assistant.  Providing a list of other stores is helpful context, but not if the core answer is wrong.\n",
      "\n",
      "--- Evaluating Test Case 4 ---\n",
      "Question: What about at Amazon?\n",
      "\n",
      "AI Response: I don't see any purchases from Amazon in your expense history.  Would you like me to check something else?\n",
      "\n",
      "Evaluation: \n",
      "Relevance: 1 - The expected answer pattern suggests the AI should analyze receipts and provide information based on them. The actual response indicates it couldn't find specific purchases, which might be relevant depending on the original question, but it doesn't follow the expected analysis of receipts.\n",
      "\n",
      "Accuracy: 5 -  We cannot determine the accuracy of the statement \"I don't see any purchases from Amazon\" without access to the expense history.  Assuming the AI has access and correctly reports its findings, it's accurate.\n",
      "\n",
      "Completeness: 2 - While the response is a complete sentence, it doesn't provide a complete answer in the context of the expected pattern.  It identifies a lack of information but doesn't offer any further analysis of other expenses.\n",
      "\n",
      "Clarity: 5 - The response is clear and easy to understand.\n",
      "\n",
      "Overall: 3 - The response is clear and potentially accurate, but it lacks relevance and completeness given the expected answer pattern. It sidesteps the core task of analyzing receipts and instead focuses on the absence of a specific vendor.  It demonstrates some helpfulness by offering to check something else.\n",
      "\n",
      "--- Evaluating Test Case 5 ---\n",
      "Question: Which category did I spend the most on?\n",
      "\n",
      "AI Response: Your top spending category is Meals, totaling 2658 AED.\n",
      "\n",
      "Evaluation: \n",
      "Relevance: 5 - The response directly addresses the implied question of what the highest spending category is.\n",
      "\n",
      "Accuracy: 5 - Assuming the provided sum of 2658 AED for Meals is correct based on the receipts, the information is factually accurate.\n",
      "\n",
      "Completeness: 4 - While it identifies the top category and its total, it could be slightly more complete by stating the time frame (e.g., \"this month,\" \"this trip\").  It fulfills the core request but could offer slightly more context.\n",
      "\n",
      "Clarity: 5 - The response is clear, concise, and easy to understand.  The language is straightforward and the information is presented effectively.\n",
      "\n",
      "Overall: 5 - The response is excellent. It effectively communicates the top spending category and its associated cost. While a minor improvement in completeness is possible, the response fulfills the core request and is easily understood.\n",
      "\n",
      "## Results Summary\n"
     ]
    }
   ],
   "source": [
    "def evaluate_response(response, expected_answer):\n",
    "    \"\"\"Evaluate a response against an expected answer using Google's Gemini model\"\"\"\n",
    "    eval_prompt = f\"\"\"\n",
    "    You are evaluating the response of an AI expense assistant against an expected answer pattern.\n",
    "    Please rate the response on a scale of 1-5 on these criteria:\n",
    "    \n",
    "    1. Relevance: Does the response directly address the question?\n",
    "    2. Accuracy: Is the information provided factually correct based on the expected answer pattern?\n",
    "    3. Completeness: Does the response provide a complete answer to the question?\n",
    "    4. Clarity: Is the response clear and easy to understand?\n",
    "    \n",
    "    Expected answer pattern: {expected_answer}\n",
    "    Actual response: {response}\n",
    "    \n",
    "    For each criterion, provide a score (1-5) and a brief explanation.\n",
    "    Then give an overall score (1-5) and summary assessment.\n",
    "    \n",
    "    Format your response as:\n",
    "    Relevance: [score] - [explanation]\n",
    "    Accuracy: [score] - [explanation]\n",
    "    Completeness: [score] - [explanation]\n",
    "    Clarity: [score] - [explanation]\n",
    "    Overall: [score] - [summary]\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation = llm.invoke(eval_prompt)\n",
    "    return evaluation.content\n",
    "\n",
    "print(\"\\n## Run Evaluation\")\n",
    "print(\"Running the evaluation on our test cases\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"\\n--- Evaluating Test Case {i+1} ---\")\n",
    "    print(f\"Question: {test_case['question']}\")\n",
    "    \n",
    "    # Generate response using the retrieval chain\n",
    "    response = retrieval_chain.invoke({\n",
    "        \"input\": test_case[\"question\"],\n",
    "        \"chat_history\": test_case[\"chat_history\"]\n",
    "    })\n",
    "    \n",
    "    actual_answer = response[\"answer\"]\n",
    "    print(f\"\\nAI Response: {actual_answer}\")\n",
    "    \n",
    "    # Extract context documents\n",
    "    context_docs = response[\"context\"]\n",
    "    context_summary = f\"Retrieved {len(context_docs)} documents\"\n",
    "    \n",
    "    # Evaluate the response\n",
    "    evaluation = evaluate_response(actual_answer, test_case[\"expected_answer\"])\n",
    "    print(f\"\\nEvaluation: \\n{evaluation}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"question\": test_case[\"question\"],\n",
    "        \"expected_answer\": test_case[\"expected_answer\"],\n",
    "        \"actual_answer\": actual_answer,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"context_summary\": context_summary\n",
    "    })\n",
    "\n",
    "print(\"\\n## Results Summary\")\n",
    "\n",
    "# Extract overall scores from evaluations\n",
    "scores = []\n",
    "for result in results:\n",
    "    eval_lines = result[\"evaluation\"].split('\\n')\n",
    "    overall_line = [line for line in eval_lines if line.startswith(\"Overall:\")]\n",
    "    if overall_line:\n",
    "        score_text = overall_line[0].split('-')[0].replace(\"Overall:\", \"\").strip()\n",
    "        try:\n",
    "            score = float(score_text)\n",
    "            scores.append(score)\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse score from: {overall_line[0]}\")\n",
    "            scores.append(None)\n",
    "    else:\n",
    "        scores.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2339802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Question\": [result[\"question\"] for result in results],\n",
    "    \"Overall Score\": scores,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c48a4f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Summary ---\n",
      "Average Score: 4.0\n",
      "Best Score: 5.0\n",
      "Worst Score: 3.0\n",
      "\n",
      "Scores by Question:\n",
      "                                          Question  Overall Score\n",
      "0  What was my most expensive purchase last month?            5.0\n",
      "1         How much did I spend on dining in total?            4.0\n",
      "2              Did I make any purchases at Target?            3.0\n",
      "3                            What about at Amazon?            3.0\n",
      "4          Which category did I spend the most on?            5.0\n",
      "\n",
      "## Detailed Analysis\n",
      "Analyzing the performance for each evaluation criterion\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "print(f\"Average Score: {sum(filter(None, scores))/len(list(filter(None, scores)))}\")\n",
    "print(f\"Best Score: {max(filter(None, scores))}\")\n",
    "print(f\"Worst Score: {min(filter(None, scores))}\")\n",
    "print(\"\\nScores by Question:\")\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\n## Detailed Analysis\")\n",
    "print(\"Analyzing the performance for each evaluation criterion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c84514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all criteria scores\n",
    "criteria_scores = {\n",
    "    \"Relevance\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Completeness\": [],\n",
    "    \"Clarity\": []\n",
    "}\n",
    "\n",
    "for result in results:\n",
    "    eval_lines = result[\"evaluation\"].split('\\n')\n",
    "    \n",
    "    for criterion in criteria_scores.keys():\n",
    "        criterion_line = [line for line in eval_lines if line.startswith(f\"{criterion}:\")]\n",
    "        if criterion_line:\n",
    "            score_text = criterion_line[0].split('-')[0].replace(f\"{criterion}:\", \"\").strip()\n",
    "            try:\n",
    "                score = float(score_text)\n",
    "                criteria_scores[criterion].append(score)\n",
    "            except ValueError:\n",
    "                print(f\"Could not parse {criterion} score from: {criterion_line[0]}\")\n",
    "                criteria_scores[criterion].append(None)\n",
    "        else:\n",
    "            criteria_scores[criterion].append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab2763c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performance by Criterion ---\n",
      "Relevance: 4.20\n",
      "Accuracy: 4.00\n",
      "Completeness: 4.00\n",
      "Clarity: 5.00\n",
      "\n",
      "## Recommendations for Improvement\n",
      "Based on the evaluation results, here are some recommendations for improving the AI Expense Assistant:\n",
      "\n",
      "--- Areas for Improvement ---\n",
      "Lowest scoring criterion: Accuracy (avg: 4.00)\n",
      "\n",
      "Lowest scoring question: 'What was my most expensive purchase last month?'\n",
      "Response: 'Last month (April 2025) your most expensive purchase was office supplies from NASCO for 400 AED.'\n",
      "\n",
      "--- Recommendations ---\n",
      "## Recommendations for Improving the AI Expense Assistant\n",
      "\n",
      "**1. Improving Accuracy (Avg: 4.00)**\n",
      "\n",
      "* **Data Integration and Validation:** Implement robust integration with all relevant expense data sources (bank accounts, credit cards, expense reports).  Validate data integrity regularly to ensure the system operates on accurate and up-to-date information.  Consider automated data cleaning processes to handle inconsistencies or missing values.\n",
      "* **Transaction Categorization Enhancement:** Refine the transaction categorization algorithm. Implement machine learning models trained on a larger, more diverse dataset of expense transactions.  Allow users to manually correct categorization errors and incorporate these corrections into the training data for continuous improvement.\n",
      "* **Currency Conversion Accuracy:** Implement real-time currency conversion using reliable APIs.  Ensure the system stores the original currency and conversion rate for transparency and auditability.\n",
      "* **Unit Testing and Regression Testing:** Implement comprehensive unit tests for all core functionalities, including data retrieval, calculation, and reporting.  Introduce regression testing to prevent new features from breaking existing functionality.\n",
      "\n",
      "\n",
      "**2. Addressing the Lowest Scoring Question: 'What was my most expensive purchase last month?'**\n",
      "\n",
      "* **Contextual Understanding of \"Expensive\":**  Allow users to specify criteria for \"expensive.\"  For example, allow filtering by category (e.g., \"most expensive travel expense\") or setting a minimum threshold (e.g., \"purchases over $500\").\n",
      "* **Handling Multiple Currencies:** If transactions are in different currencies, clarify the currency used for determining \"most expensive.\"  Provide the option to see the most expensive purchase in each currency or converted to a preferred currency.\n",
      "* **Edge Case Handling:**  Address edge cases like refunds, returns, or cancelled transactions. These should not be considered as \"purchases\" when determining the most expensive item.\n",
      "* **Improved Response Format:**  Provide a more informative response.  Instead of just stating the amount and vendor, include the date of the purchase, transaction ID, and a link to the original transaction details.  Example: \"Your most expensive purchase last month (April 2025) was office supplies from NASCO for 400 AED on April 15th (Transaction ID: XYZ123). [Link to transaction details]\"\n",
      "\n",
      "\n",
      "**3. Improving the Example Response: 'Last month (April 2025) your most expensive purchase was office supplies from NASCO for 400 AED.'**\n",
      "\n",
      "* **Dynamic Date Handling:** Instead of hardcoding \"April 2025,\" dynamically determine and display the previous month based on the current date.\n",
      "* **Category Specificity:**  \"Office supplies\" is a broad category.  If possible, provide more specific information, e.g., \"printer paper,\" \"pens,\" or \"desk organizers.\" This requires improved transaction categorization as mentioned above.\n",
      "* **Link to Supporting Information:** Include a link to the original transaction details or expense report for verification and further investigation.\n",
      "\n",
      "\n",
      "**4. General Recommendations for the System as a Whole:**\n",
      "\n",
      "* **User Interface/User Experience (UI/UX) Improvements:**  Focus on a clean, intuitive interface that is easy to navigate and understand.  Conduct user testing to identify areas for improvement.\n",
      "* **Personalization:** Allow users to customize the system to their specific needs and preferences, such as setting budget alerts, preferred reporting formats, and expense categories.\n",
      "* **Proactive Insights:**  Move beyond reactive responses to user queries.  Provide proactive insights, such as spending trends, budget overruns, and potential savings opportunities.\n",
      "* **Multi-Language Support:**  Consider supporting multiple languages to cater to a wider user base.\n",
      "* **Security and Privacy:**  Ensure the system adheres to the highest security and privacy standards to protect sensitive financial data.\n",
      "\n",
      "\n",
      "By implementing these recommendations, the AI Expense Assistant can significantly improve its accuracy, usability, and overall value to users.  Regular monitoring and evaluation are crucial for continuous improvement and adaptation to evolving user needs.\n",
      "\n",
      "## Context Analysis\n",
      "Looking at how well the system is retrieving relevant context\n",
      "Question: What was my most expensive purchase last month?\n",
      "\n",
      "Retrieved 5 documents\n",
      "\n",
      "Context Relevance Analysis:\n",
      "* **Expense at H&M for 169 AED on 2022-12-23 under Others category. Description: ...** Relevance: 1.  This expense is from December 2022, not last month (relative to the hypothetical present when the question is asked, which we must assume is sometime in 2025 based on the other entries).  Therefore, it's irrelevant.\n",
      "\n",
      "* **Expense at Faber Castle for 2700 AED on 2025-03-11 under Office Supplies category. Description: Office Supplies for home....** Relevance: 2. This expense is from March 2025.  If \"last month\" is April 2025, then this is relevant as a *potential* candidate for the most expensive purchase. However, we don't know if it *is* the most expensive without seeing other March expenses.\n",
      "\n",
      "* **Expense at Paragon for 2500 AED on 2025-03-11 under Meals category. Description: Meal with family....** Relevance: 2.  Same reasoning as the Faber Castle expense.  Potentially relevant, but not definitively.\n",
      "\n",
      "* **Expense at NASCO for 400 AED on 2025-04-15 under Office Supplies category. Description: table...** Relevance: 4. This expense is from April 2025, making it a strong candidate for \"last month's\" expenses.  It's not a 5 because we still need to know if there were other, more expensive purchases in April.\n",
      "\n",
      "* **Expense at Gym for 3000 AED on 2025-03-11 under Training category. Description: Gym training to get six packs....** Relevance: 2. Same reasoning as the Faber Castle and Paragon expenses.\n",
      "\n",
      "\n",
      "**Overall Assessment and Suggestions for Improvement:**\n",
      "\n",
      "The retrieval quality is poor.  The system isn't effectively filtering by date.  The question explicitly asks for \"last month's\" most expensive purchase, yet the retrieved documents include expenses from December 2022 and March 2025.  This suggests the retrieval system isn't understanding the temporal aspect of the query.\n",
      "\n",
      "Here are some suggestions for improvement:\n",
      "\n",
      "* **Improved Date Filtering:** The retrieval system needs to prioritize accurate date filtering.  It should first identify the date range corresponding to \"last month\" relative to the current date (or a specified date if provided in the query) and only retrieve expenses within that range.\n",
      "\n",
      "* **Ranking by Price:** Within the correct date range, the results should be ranked by price in descending order, so the most expensive purchase appears first.\n",
      "\n",
      "* **Consider \"Most Expensive\" Explicitly:** The system should understand the superlative \"most expensive\" and return only the single highest expense for the specified period, or at least clearly indicate which expense is the highest.\n",
      "\n",
      "* **Contextual Understanding of \"Month\":** The system should understand variations in how \"month\" might be expressed (e.g., \"last month,\" \"previous month,\" \"March,\" \"April 2025\").\n",
      "\n",
      "* **Error Handling for Ambiguous Queries:** If the query is ambiguous about the relevant time period (e.g., if the user simply asks \"What was my most expensive purchase?\"), the system should either ask clarifying questions or provide results with clear date information so the user can easily identify the relevant time frame.\n",
      "\n",
      "## Conclusion\n",
      "This evaluation has assessed the AI Expense Assistant on multiple criteria across various test cases.\n",
      "Based on the results, we've identified strengths, weaknesses, and areas for improvement in the system.\n"
     ]
    }
   ],
   "source": [
    "# Create a summary of criterion averages\n",
    "criterion_averages = {}\n",
    "for criterion, scores in criteria_scores.items():\n",
    "    filtered_scores = list(filter(None, scores))\n",
    "    if filtered_scores:\n",
    "        criterion_averages[criterion] = sum(filtered_scores) / len(filtered_scores)\n",
    "    else:\n",
    "        criterion_averages[criterion] = None\n",
    "\n",
    "print(\"\\n--- Performance by Criterion ---\")\n",
    "for criterion, avg in criterion_averages.items():\n",
    "    print(f\"{criterion}: {avg:.2f}\")\n",
    "\n",
    "print(\"\\n## Recommendations for Improvement\")\n",
    "print(\"Based on the evaluation results, here are some recommendations for improving the AI Expense Assistant:\")\n",
    "\n",
    "# Find the lowest-scoring criterion\n",
    "lowest_criterion = min(criterion_averages.items(), key=lambda x: x[1] if x[1] is not None else float('inf'))\n",
    "\n",
    "# Find the lowest-scoring question\n",
    "lowest_question_idx = scores.index(min(filter(None, scores)))\n",
    "lowest_question = results[lowest_question_idx][\"question\"]\n",
    "lowest_answer = results[lowest_question_idx][\"actual_answer\"]\n",
    "\n",
    "print(f\"\\n--- Areas for Improvement ---\")\n",
    "print(f\"Lowest scoring criterion: {lowest_criterion[0]} (avg: {lowest_criterion[1]:.2f})\")\n",
    "print(f\"\\nLowest scoring question: '{lowest_question}'\")\n",
    "print(f\"Response: '{lowest_answer}'\")\n",
    "\n",
    "improvement_prompt = f\"\"\"\n",
    "Based on the evaluation results, please provide specific recommendations for improving the AI Expense Assistant. Focus on:\n",
    "\n",
    "1. The lowest scoring criterion: {lowest_criterion[0]} (avg: {lowest_criterion[1]:.2f})\n",
    "2. The lowest scoring question: '{lowest_question}' \n",
    "3. Ways to improve the response: '{lowest_answer}'\n",
    "4. General recommendations for the system as a whole\n",
    "\n",
    "Format your response as specific, actionable recommendations.\n",
    "\"\"\"\n",
    "\n",
    "recommendations = llm.invoke(improvement_prompt)\n",
    "print(f\"\\n--- Recommendations ---\\n{recommendations.content}\")\n",
    "\n",
    "print(\"\\n## Context Analysis\")\n",
    "print(\"Looking at how well the system is retrieving relevant context\")\n",
    "\n",
    "# Run a detailed analysis of retrieved context for the first test case\n",
    "test_response = retrieval_chain.invoke({\n",
    "    \"input\": test_cases[0][\"question\"],\n",
    "    \"chat_history\": test_cases[0][\"chat_history\"]\n",
    "})\n",
    "\n",
    "print(f\"Question: {test_cases[0]['question']}\")\n",
    "print(f\"\\nRetrieved {len(test_response['context'])} documents\")\n",
    "\n",
    "context_analysis_prompt = f\"\"\"\n",
    "Analyze how relevant the retrieved context is to the question: '{test_cases[0]['question']}'\n",
    "\n",
    "Context documents retrieved:\n",
    "{[doc.page_content[:200] + '...' for doc in test_response['context']]}\n",
    "\n",
    "For each document snippet, rate its relevance to the question on a scale of 1-5 (5 being most relevant).\n",
    "Then provide an overall assessment of the retrieval quality and specific suggestions for improvement.\n",
    "\"\"\"\n",
    "\n",
    "context_analysis = llm.invoke(context_analysis_prompt)\n",
    "print(f\"\\nContext Relevance Analysis:\\n{context_analysis.content}\")\n",
    "\n",
    "print(\"\\n## Conclusion\")\n",
    "print(\"This evaluation has assessed the AI Expense Assistant on multiple criteria across various test cases.\")\n",
    "print(\"Based on the results, we've identified strengths, weaknesses, and areas for improvement in the system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe81e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
